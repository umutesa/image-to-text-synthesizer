# -*- coding: utf-8 -*-
"""Diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G-dyvNZH-jABIRiuNUbD2twcuqPIbR6P
"""

!pip install diffusers --upgrade
!pip install invisible_watermark transformers accelerate safetensors

#-----------------------------IMPORT LIBRARIES---------------------------------
import torch
from diffusers import DiffusionPipeline

import cv2
from PIL import Image
import matplotlib.pyplot as plt
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import cifar10

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

#------------------------------------DEFINE MODEL-----------------------------------------------

model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning") #load pre-trained model from hugging face ; Vision Transformer (ViT) to understand the image and a GPT-2 model to generate the caption
feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning") # Get Image Feature Extractor
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning") # Get Tokenizer Mode

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#----------------------------------PREDICT STEP FOR IMAGES-------------------------------------------------------

max_length = 16  # The maximum length the generated tokens can have
num_beams = 4 #  Number of beams for beam search
gen_kwargs = {"max_length": max_length, "num_beams": num_beams} # Generation Config

def predict_step(images):  # predict list of  images
    pil_images = []
    for img_array in images:
        img = Image.fromarray(img_array.astype('uint8'), 'RGB')
        pil_images.append(img)

    pixel_values = feature_extractor(images=pil_images, return_tensors="pt").pixel_values  # Feature extractor
    pixel_values = pixel_values.to(device)

    output_ids = model.generate(pixel_values, **gen_kwargs)  # Apply model

    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True) # Get text tokens description of image
    preds = [pred.strip() for pred in preds]
    return preds

#------------------------LOAD DATASET AND TEST MODEL---------------------------

(x_train, y_train), (x_test, y_test) = cifar10.load_data() # Load CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck'] #Labels

idx = np.random.randint(0, len(x_train)) # Pick a random index
img_array = x_train[idx]  # Input shape of dataset (32, 32, 3)- input shape of dataset
label = class_names[y_train[idx][0]]

img = Image.fromarray(img_array) # Convert to high-quality image

plt.imshow(img) # Display the image
plt.title(f"Random CIFAR-10 Image: {label}")
plt.axis('off')
plt.show()
print(predict_step([x_train[idx]]))

#-----------------------------VALIDATION--------------------------------------------------

num_samples = 5
x_sample = x_test[:num_samples]
y_true = y_test[:num_samples].flatten()
y_pred = predict_step(x_sample)
true_labels = [class_names[i] for i in y_true]


fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))
for i in range(num_samples):
    img = x_sample[i]

    # True label
    axes[i, 0].imshow(img)
    axes[i, 0].axis('off')
    axes[i, 0].set_title(f"True: {true_labels[i]}", color='green')

    # Predicted label
    axes[i, 1].imshow(img)
    axes[i, 1].axis('off')
    axes[i, 1].set_title(f"Predicted: {y_pred[i]}", color='red')

plt.show()